{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:50px;\">Recommendation-System-for-Book-Crossing-Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:25pt;\"> Introduction</h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\"> In this notebook, by using some techniques we will perform Data Cleansing and some Exploratory Analysis on the <a href=\"http://www2.informatik.uni-freiburg.de/~cziegler/BX/\">Book-Crossing Dataset</a> collected by Cai-Nicolas Ziegler. The reason is to prepare the data in a usable format and also, to gain some intuition about the data. By this, we can know the best model to use in building the recommender system to provide book-crossing recommendations. </p>\n",
    "\n",
    " <img src=\"Data/images/data-cleansing-1.jpg\" width=\"300\" height=\"300\"> \n",
    "<p></p>\n",
    "<p style=\"color:red;font-size:15pt;\">The dataset consists of three files.</p>\n",
    " <ul style=\"font-size:20px\">\n",
    "  <li><em><b>BX_Users.csv</b></em> containing data from 278858 users (User Id, Location, Age).</li>\n",
    "  <li><em><b>BX_Books.csv</b></em> has information for 271379 books including Title, Author, Year of Publication, Publisher and Cover images.</li>\n",
    "  <li><em><b>BX_Book_Ratings.csv.</b></em> holding 1149780 ratings from the users.</li>\n",
    "</ul>\n",
    "<hr>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim \n",
    "import tqdm   #to visualize loops' progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt\">I downloaded the data in csv format, so we are going to use <i>pandas.read_csv()</i> to create three DataFrames.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import Data from csv\n",
    "# read Users\n",
    "u_cols = ['user_id', 'location', 'age']\n",
    "users = pd.read_csv('Data\\datasets\\BX_Users.csv', sep=';', names=u_cols, encoding='latin-1',low_memory=False)\n",
    "\n",
    "# read Books/items\n",
    "i_cols = ['isbn', 'book_title' ,'book_author','year_of_publication', 'publisher', 'img_s', 'img_m', 'img_l']\n",
    "items = pd.read_csv('Data\\datasets\\BX_Books.csv', sep=';', names=i_cols, encoding='latin-1',low_memory=False)\n",
    "\n",
    "# read Ratings\n",
    "r_cols = ['user_id', 'isbn', 'rating']\n",
    "ratings = pd.read_csv('Data\\datasets\\BX_Book_Ratings.csv', sep=';', names=r_cols, encoding='latin-1',low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<p style=\"font-size:15pt\">Now let's print the 3 first elements of each dataframe we created</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________Users__________________________________\n",
      "   user_id                   location  age\n",
      "0  User-ID                   Location  Age\n",
      "1        1         nyc, new york, usa  NaN\n",
      "2        2  stockton, california, usa   18\n",
      "\n",
      "\n",
      "__________________________________Items__________________________________\n",
      "         isbn           book_title           book_author  year_of_publication  \\\n",
      "0        ISBN           Book-Title           Book-Author  Year-Of-Publication   \n",
      "1  0195153448  Classical Mythology    Mark P. O. Morford                 2002   \n",
      "2  0002005018         Clara Callan  Richard Bruce Wright                 2001   \n",
      "\n",
      "                 publisher                                              img_s  \\\n",
      "0                Publisher                                        Image-URL-S   \n",
      "1  Oxford University Press  http://images.amazon.com/images/P/0195153448.0...   \n",
      "2    HarperFlamingo Canada  http://images.amazon.com/images/P/0002005018.0...   \n",
      "\n",
      "                                               img_m  \\\n",
      "0                                        Image-URL-M   \n",
      "1  http://images.amazon.com/images/P/0195153448.0...   \n",
      "2  http://images.amazon.com/images/P/0002005018.0...   \n",
      "\n",
      "                                               img_l  \n",
      "0                                        Image-URL-L  \n",
      "1  http://images.amazon.com/images/P/0195153448.0...  \n",
      "2  http://images.amazon.com/images/P/0002005018.0...  \n",
      "\n",
      "\n",
      "_________________________________Ratings_________________________________\n",
      "   user_id        isbn       rating\n",
      "0  User-ID        ISBN  Book-Rating\n",
      "1   276725  034545104X            0\n",
      "2   276726  0155061224            5\n"
     ]
    }
   ],
   "source": [
    "print('__________________________________Users__________________________________')\n",
    "print(users.head(3))\n",
    "print(\"\\n\")\n",
    "print('__________________________________Items__________________________________')\n",
    "print(items.head(3))\n",
    "print(\"\\n\")\n",
    "print('_________________________________Ratings_________________________________')\n",
    "print(ratings.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p><p style=\"font-size:15pt\">From the result above, we can see that the first line of each Dataframe (Df) contains the column names. We shall remove them and then reset the index of the Dataframes. <br><br>Moreso, Df <b>\"Items\"</b> has 3 columns of image URLs, we will get rid of them because we are not going to need them.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________Users__________________________________\n",
      "  user_id                            location  age\n",
      "0       1                  nyc, new york, usa  NaN\n",
      "1       2           stockton, california, usa   18\n",
      "2       3     moscow, yukon territory, russia  NaN\n",
      "3       4           porto, v.n.gaia, portugal   17\n",
      "4       5  farnborough, hants, united kingdom  NaN\n",
      "\n",
      "\n",
      "__________________________________Items__________________________________\n",
      "         isbn                                         book_title  \\\n",
      "0  0195153448                                Classical Mythology   \n",
      "1  0002005018                                       Clara Callan   \n",
      "2  0060973129                               Decision in Normandy   \n",
      "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
      "4  0393045218                             The Mummies of Urumchi   \n",
      "\n",
      "            book_author year_of_publication                   publisher  \n",
      "0    Mark P. O. Morford                2002     Oxford University Press  \n",
      "1  Richard Bruce Wright                2001       HarperFlamingo Canada  \n",
      "2          Carlo D'Este                1991             HarperPerennial  \n",
      "3      Gina Bari Kolata                1999        Farrar Straus Giroux  \n",
      "4       E. J. W. Barber                1999  W. W. Norton &amp; Company  \n",
      "\n",
      "\n",
      "_________________________________Ratings_________________________________\n",
      "  user_id        isbn rating\n",
      "0  276725  034545104X      0\n",
      "1  276726  0155061224      5\n",
      "2  276727  0446520802      0\n",
      "3  276729  052165615X      3\n",
      "4  276729  0521795028      6\n"
     ]
    }
   ],
   "source": [
    "users = users.loc[1:]\n",
    "users.reset_index(drop=True, inplace=True)\n",
    "\n",
    "items = items.loc[1:]\n",
    "items.reset_index(drop=True, inplace=True)\n",
    "items.drop(['img_s','img_m','img_l'], axis=1, inplace=True)\n",
    "\n",
    "ratings = ratings.loc[1:]\n",
    "ratings.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('__________________________________Users__________________________________')\n",
    "print(users.head(5))\n",
    "print(\"\\n\")\n",
    "print('__________________________________Items__________________________________')\n",
    "print(items.head(5))\n",
    "print(\"\\n\")\n",
    "print('_________________________________Ratings_________________________________')\n",
    "print(ratings.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p><p style=\"font-size:15pt\">Now let's use DataFrame.describe() function to see what else we can find about our data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id                         location     age\n",
      "count   278858                           278858  168096\n",
      "unique  278858                            57339     165\n",
      "top     257602  london, england, united kingdom      24\n",
      "freq         1                             2506    5687 \n",
      "\n",
      "\n",
      "              isbn      book_title      book_author year_of_publication  \\\n",
      "count       271379          271379           271378              271379   \n",
      "unique      271379          242154           102042                 137   \n",
      "top     155773058X  Selected Poems  Agatha Christie                2002   \n",
      "freq             1              27              632               17627   \n",
      "\n",
      "        publisher  \n",
      "count      271377  \n",
      "unique      16824  \n",
      "top     Harlequin  \n",
      "freq         7535   \n",
      "\n",
      "\n",
      "        user_id        isbn   rating\n",
      "count   1149780     1149780  1149780\n",
      "unique   105283      340556       11\n",
      "top       11676  0971880107        0\n",
      "freq      13602        2502   716109\n"
     ]
    }
   ],
   "source": [
    "print(users.describe(),\"\\n\"*2)\n",
    "print(items.describe(),\"\\n\"*2)\n",
    "print(ratings.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt\">From the above, we can see that\n",
    "<ul style=\"font-size:12pt\">\n",
    "<li>Age is not numerical. We'll have to convert it to <em>int</em></li>\n",
    "<li>Year of publication is also not numerical. We'll have to convert it to <em>int</em> as well</li>\n",
    "<li>I also guess that there are some duplicate books in there.</li>\n",
    "<li>We have <em>ratings</em> for 340556 different book titles. This is more than the actual books in \"items\" Df</li>\n",
    "<li>The <em>ratings</em> come from 105283 different <em>users</em></li> \n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:12pt\"> </p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:25pt;\"> <u>Users DataFrame</u></h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">Now, we take a closer look at the <em>\"users\"</em> Df.</p>\n",
    "<p style=\"font-size:15pt\"> Let's convert users.user_id to 'int' (we'll need in future) and users.age to 'float' ('int' will also be a good solution but since there are numerous NaN values, we have to go with 'float' first.) After conversion, we call df.describe() again.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>location</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>278858.00000</td>\n",
       "      <td>278858</td>\n",
       "      <td>168096.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>57339</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>london, england, united kingdom</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2506</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>139429.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.751434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>80499.51502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.428097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>69715.25000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>139429.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>209143.75000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>278858.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>244.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id                         location            age\n",
       "count   278858.00000                           278858  168096.000000\n",
       "unique           NaN                            57339            NaN\n",
       "top              NaN  london, england, united kingdom            NaN\n",
       "freq             NaN                             2506            NaN\n",
       "mean    139429.50000                              NaN      34.751434\n",
       "std      80499.51502                              NaN      14.428097\n",
       "min          1.00000                              NaN       0.000000\n",
       "25%      69715.25000                              NaN      24.000000\n",
       "50%     139429.50000                              NaN      32.000000\n",
       "75%     209143.75000                              NaN      44.000000\n",
       "max     278858.00000                              NaN     244.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.age = users.age.astype(float)\n",
    "users.user_id = users.user_id.astype(int)\n",
    "users.describe(include=[object, int, float])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><h2 style=\"font-size:20pt;\"> users.age</h2>\n",
    "\n",
    "<p style=\"font-size:15pt\"> Taking a closer look at \"users.age\" Series,</p> \n",
    "<p style=\"font-size:15pt\"> We can see from the above that there are 110762 missing values in <em>users.age</em>. Also, it has a mean value around 34.75 and a deviation of 14.43.</p> \n",
    "<p style=\"font-size:15pt\"> Next, we shall investigate how many NaN values there are in the Series and how many values are \"not so logical\" (> 5 or < 99).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in age: 0\n",
      "Users with 5 > age & age > 99 : 1255\n"
     ]
    }
   ],
   "source": [
    "print(\"NaN values in age:\", users.age[users.age.isna()].count())\n",
    "print(\"Users with 5 > age & age > 99 :\",users.loc[(users.age>99) | (users.age<5),'age'].count())\n",
    "users.loc[(users.age>99) | (users.age<5),'age'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt\"><b>Wow</b>, 110762 missing values in <em>users.age</em> and another 1255 users have <i>age</i> values that I think are invalid. <br></br>Since we really don't like NaN values in datasets we will replace them with a value. This value can be the mean age of the users that registered their age or it can be based on some values of the dataset. For example, we can use the location of the user and use the average age of the population of this location, or we can use the average age of the population using PCs on that location since this dataset was collected from a website. We can even use an ML algorithm to determine the missing values. The easiest way  would be to use the mean.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    278858.000000\n",
       "mean         34.743900\n",
       "std          10.540292\n",
       "min           5.000000\n",
       "25%          29.000000\n",
       "50%          34.743900\n",
       "75%          35.000000\n",
       "max          99.000000\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.age.fillna(users.age.mean()).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt\">But if we do the above, we would notice that the deviation changed a lot, 3.1 units. And since we don't want to create a huge group of 112017 users with the same age, we are going to try something different.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of values in 'users.age'\n",
      " count    166841.000000\n",
      "mean         34.743900\n",
      "std          13.626783\n",
      "min           5.000000\n",
      "25%          24.000000\n",
      "50%          32.000000\n",
      "75%          44.000000\n",
      "max          99.000000\n",
      "Name: age, dtype: float64 \n",
      "\n",
      "Statistics of values we are going to use to fill NaN \n",
      " count    112017.000000\n",
      "mean         34.722537\n",
      "std          13.656769\n",
      "min         -26.598528\n",
      "25%          25.542527\n",
      "50%          34.748836\n",
      "75%          43.981557\n",
      "max          92.565994\n",
      "dtype: float64 \n",
      "\n",
      "Negative values in 'temp_age_seires': 603 \n",
      "\n",
      "As we can see the destribution doesnt change a lot. There are some negative values thought (around 600 of them).\n",
      "\n",
      "count    278858.000000\n",
      "mean         34.926317\n",
      "std          13.387361\n",
      "min           5.000000\n",
      "25%          25.000000\n",
      "50%          33.000000\n",
      "75%          44.000000\n",
      "max          99.000000\n",
      "Name: age, dtype: float64 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>location</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc, new york, usa</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton, california, usa</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow, yukon territory, russia</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto, v.n.gaia, portugal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough, hants, united kingdom</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                            location  age\n",
       "0        1                  nyc, new york, usa   51\n",
       "1        2           stockton, california, usa   18\n",
       "2        3     moscow, yukon territory, russia   27\n",
       "3        4           porto, v.n.gaia, portugal   17\n",
       "4        5  farnborough, hants, united kingdom   47"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a normal distribution pd.Series to fill Nan values with\n",
    "temp_age_series = pd.Series(np.random.normal(loc=users.age.mean(), scale=users.age.std(), size=users.user_id[users.age.isna()].count()))\n",
    "print(\"Statistics of values in \\'users.age\\'\\n\",users.age.describe(),\"\\n\")\n",
    "print(\"Statistics of values we are going to use to fill NaN \\n\",temp_age_series.describe(),\"\\n\")\n",
    "print(\"Negative values in \\'temp_age_seires\\':\", temp_age_series[temp_age_series<0].count(),\"\\n\")\n",
    "print(\"As we can see the destribution doesnt change a lot. There are some negative values thought (around 600 of them).\\n\")\n",
    "\n",
    "# take the abs value of temp_age_series\n",
    "pos_age_series=np.abs(temp_age_series)\n",
    "\n",
    "# sort users Df so as NaN values in age to be first and reset index to match with index of pos_age_series. Then use fillna()\n",
    "users = users.sort_values('age',na_position='first').reset_index(drop=True)\n",
    "users.age.fillna(pos_age_series, inplace = True)  \n",
    "\n",
    "# replace values < 5 with the mean(). Round values and convert them to int. \n",
    "users.loc[users.age<5, 'age'] = users.age.mean()\n",
    "users.age = users.age.round().astype(int)\n",
    "#Sort users based on user_id so as to be the same as before\n",
    "users = users.sort_values('user_id').reset_index(drop=True)\n",
    "print(users.age.describe(),\"\\n\")\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><p style=\"font-size:15pt\">Finally, we have to decide if we are going to let users.age as a numerical (quantitative) data or if we should change it to categorical (qualitative). If we choose \"numerical\" we add a continuity to the data. It might, for example, show that people read more books or rate them lower as they age. On the other hand, if we choose to change it to categorical then it will be better to talk about \"age group\" and we will treat the users of each group as a class.<br><br>I am going to leave it as it is for now (numerical). In case you like to make it categorical all you have to do is to change each users age with a group (e.g {if (users.age>5)&(users.age< 15) then '5-15'} and so on ) and then use pd.get_dummies() to get the one-hot encoding of it.</p>  <p style=\"font-size:15pt\"> Thanks to PascPeli for this.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><h2 style=\"font-size:20pt;\"> users.location</h2>\n",
    "<p style=\"font-size:15pt\">Continuing on with the <i>\"users.location\"</i> Series. It is formatted as a long string containing three comma-separated instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    nyc, new york, usa\n",
       "1             stockton, california, usa\n",
       "2       moscow, yukon territory, russia\n",
       "3             porto, v.n.gaia, portugal\n",
       "4    farnborough, hants, united kingdom\n",
       "Name: location, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<br></br><p style=\"font-size:15pt\">I believe that it would be more insightful to separate those strings. So, I suggest we should split the <i>\"users.location\"</i> in three different columns: <i>\"city\"</i>, <i>\"state\"</i> and <i>\"country\"</i> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>278858</td>\n",
       "      <td>278857</td>\n",
       "      <td>274281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>33076</td>\n",
       "      <td>6663</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>london</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4105</td>\n",
       "      <td>19839</td>\n",
       "      <td>139421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city       state country\n",
       "count   278858      278857  274281\n",
       "unique   33076        6663    1130\n",
       "top     london  california     usa\n",
       "freq      4105       19839  139421"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_split=users.location.str.split(', ', n=2, expand=True)\n",
    "location_split.columns=['city', 'state', 'country']\n",
    "location_split.describe(include=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<p style=\"font-size:15pt;\">Having done that, we can easily detect invalid or NaN values. There are users with <em>\"state\" == ','</em>  and <em>\"country\"== None</em> . We are going to replace those values with 'other'.<br>There are also some users with ' ', '\\\\n/a\\\\\"', 'n.a' or '*' as a \"state\" value, we'll replace those with 'n/a'</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_split.loc[location_split.state==',', ['state', 'country']] = 'other'\n",
    "location_split.loc[location_split.country==',', ['country']] = 'other'\n",
    "location_split.loc[(location_split.state=='\\\\n/a\\\\\"') | (location_split.state=='') | (location_split.state=='*') | (location_split.state=='n.a'), ['state']] = 'n/a'\n",
    "location_split.state.fillna('other', inplace=True)\n",
    "location_split.fillna('n/a', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">All those different values would increase the dimensionality tremendously if we were to use One-Hot encoding. In an attempt to tackle this we can start replacing not so frequent values with 'other'. But that will lead us to lose quite a lot of data (depending on the \"keep threshold\"). And even with that, the resulting One-Hot encoding columns will be equal to the [No of unique strings in \"location_split\" = 40864 (after the previous operations)].\n",
    "<br><br>Instead, I suggest we transform all those locations to vectors (Word embeddings). To do this, we will train a <a href=\"https://en.wikipedia.org/wiki/Word2vec\">Word2Vec model</a> and we will use the location values as input. In order to do this we will use the Gensim library implementation of the <a href=\"https://radimrehurek.com/gensim/models/word2vec.html\">model</a>. In this way, we will be able to replace the three columns with <b>3*n</b> new columns, where <b>n</b> is the size of the Word2Vec model Neural Network.\n",
    "<br>This might not be the conventional usage of a word embedding model but good for keeping the dimensions low, it also helps in case we want to classify the users based on location.</p> <p style=\"font-size:15pt\"> Thanks to PascPeli for this.</p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_location_df = pd.concat([location_split.city, location_split.state,  location_split.country, location_split.state, location_split.city, location_split.country, location_split.city], axis=1)\n",
    "location_list = temp_location_df.fillna('n/a').values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">The selected parameters of the model work well, though you can change it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK is to Milton Keynes what Greece is to : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('attiki', 0.9594506025314331),\n",
       " ('drama', 0.9445879459381104),\n",
       " ('patra', 0.9436532855033875),\n",
       " ('maroussi', 0.9399164319038391),\n",
       " ('glyphada', 0.9276916980743408),\n",
       " ('korydalos', 0.9239610433578491),\n",
       " ('attika', 0.9236511588096619),\n",
       " ('nikaia', 0.9201735854148865),\n",
       " ('patras', 0.9198117256164551),\n",
       " ('ioannina', 0.9195963144302368),\n",
       " ('attica', 0.9173970222473145),\n",
       " ('beaumont', 0.916610598564148),\n",
       " ('magnisia', 0.9132117033004761),\n",
       " ('volos', 0.9125012159347534),\n",
       " ('stephenville', 0.9108563661575317),\n",
       " ('glyfada', 0.9102470874786377),\n",
       " ('iraklion', 0.9067165851593018),\n",
       " ('piraeus', 0.9052588939666748),\n",
       " ('boulong billancourt', 0.9043937921524048),\n",
       " ('markopoulo', 0.903727650642395)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "model = gensim.models.Word2Vec(location_list, size= n, window=3, min_count=1, workers=4)\n",
    "print ('UK is to Milton Keynes what Greece is to : ')\n",
    "model.wv.most_similar(positive=['greece','united kingdom'], negative=['milton keynes'], topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:13pt;\">With most of them being either cities or states of Greece.</p>\n",
    "<p style=\"font-size:15pt;\">The cell below constructs the 'location_vec' Df where \"city, \"state\" and \"country\" are represented with their respective vectors calculated with Word2Vec model.</p>\n",
    "<p style=\"font-size:13pt; color:green\">The operations that follow may require a substantial amount of time. If you choose to skip it, it will not affect the rest of the notebook.</p>\n",
    "<p style=\"font-size:15pt\"> Thanks to PascPeli for the code below.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose '1' to skip this step or '2' to construct the 'location_vec' DataFrame.1\n",
      "Skipping operations\n"
     ]
    }
   ],
   "source": [
    "rightchoice=['1','2']\n",
    "choice = input(\"Choose \\'1\\' to skip this step or \\'2\\' to construct the \\'location_vec\\' DataFrame.\")\n",
    "while choice not in rightchoice:\n",
    "    choice = input(\"Wrong input. \\n Insert a number. Either 1 or 2\")\n",
    "if choice=='1':\n",
    "    print ('Skipping operations')\n",
    "else:\n",
    "    zipp = list(zip(model.wv.index2word, model.wv.syn0))\n",
    "    vectors = np.zeros((location_split.shape[0],3*n))\n",
    "    for i in tqdm.tqdm_notebook(range(location_split.shape[0])):\n",
    "        vectors[i, 0:20] = [j[1][0] for j in zipp if j[0] == location_split.loc[i, 'city']]\n",
    "        vectors[i,20:40] = [j[1][0] for j in zipp if j[0] == location_split.loc[i, 'state']]\n",
    "        vectors[i,40:60] = [j[1][0] for j in zipp if j[0] == location_split.loc[i, 'country']]\n",
    "    col=[]\n",
    "    for i in range(20):\n",
    "        col.append('city_'+ str(i))\n",
    "    for i in range(20):\n",
    "        col.append('state_'+ str(i))\n",
    "    for i in range(20):\n",
    "        col.append('country_'+ str(i))\n",
    "\n",
    "    location_vec = pd.DataFrame(vectors, columns = col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:20pt;\"> users_new</h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">Finally, create a new DataFrame named \"users_new\". The new will contain</p> \n",
    "<ul style=\"font-size:13pt;\">\n",
    "the \"users.id\", <br>the \"location_vec\" if the Df exist or the \"location_spit\" otherwise <br>and the \"users.age\".\n",
    "</ul>\n",
    "<p style=\"font-size:12pt; color:green\">Replace \"users.age\" with pd.get_dummies(users.age) if you chose age to be categorical.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nyc</td>\n",
       "      <td>new york</td>\n",
       "      <td>usa</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stockton</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>moscow</td>\n",
       "      <td>yukon territory</td>\n",
       "      <td>russia</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>porto</td>\n",
       "      <td>v.n.gaia</td>\n",
       "      <td>portugal</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>farnborough</td>\n",
       "      <td>hants</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id         city            state         country  age\n",
       "0        1          nyc         new york             usa   51\n",
       "1        2     stockton       california             usa   18\n",
       "2        3       moscow  yukon territory          russia   27\n",
       "3        4        porto         v.n.gaia        portugal   17\n",
       "4        5  farnborough            hants  united kingdom   47"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'location_vec' in globals():\n",
    "    users_new = pd.concat([users.user_id, location_vec , users.age], axis=1)    \n",
    "else:\n",
    "    users_new = pd.concat([users.user_id, location_split , users.age], axis=1)\n",
    "users_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">One last thing to investigate in \"users\" before we move on to the next dataframe is the number of users that have rated at least one item. So...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105283 users have submited at least one review\n",
      "173575 users have not submited any review\n"
     ]
    }
   ],
   "source": [
    "print(users_new[users_new.user_id.isin(ratings.user_id)].user_id.count(),'users have submited at least one review')\n",
    "print(users_new[~users_new.user_id.isin(ratings.user_id)].user_id.count(), 'users have not submited any review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:25pt;\"> <u>Items DataFrame</u></h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">As mentioned before, <em>\"items\"</em> Df consists of the books' <em>isbn, title, author, year of publication and publisher</em>.<br><br> It would be a good start to convert the values in \"year_of_publication\" to int. When we try that we get the following error.\n",
    "<img src=\"Data/images/items_as(int)_error.jpg\" width=\"900\" height=\"150\">\n",
    "That indicates that some values got mixed up. I opened the csv in a text editor and corrected the few entries that were messed up and stored the file as \"BX_Books_correct.csv\". It turns out that most of the errors were caused by <em style=\"color:green\">\"&\"</em> character been written as <em style=\"color:red\">\"&</em><em style=\"font-size:4px;\"> </em><em style=\"color:red\">amp;\"</em> with the semicolon causing a string split at the wrong position of the csv file. There were also some other entries that contained semicolons at the \"book_title\" or the \"book_author\".</p> \n",
    "\n",
    "<p style=\"font-size:15pt;\">So, let's import the corrected csv.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0195153448</td>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991</td>\n",
       "      <td>HarperPerennial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0393045218</td>\n",
       "      <td>The Mummies of Urumchi</td>\n",
       "      <td>E. J. W. Barber</td>\n",
       "      <td>1999</td>\n",
       "      <td>W. W. Norton &amp; Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         isbn                                         book_title  \\\n",
       "0  0195153448                                Classical Mythology   \n",
       "1  0002005018                                       Clara Callan   \n",
       "2  0060973129                               Decision in Normandy   \n",
       "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "4  0393045218                             The Mummies of Urumchi   \n",
       "\n",
       "            book_author  year_of_publication                publisher  \n",
       "0    Mark P. O. Morford                 2002  Oxford University Press  \n",
       "1  Richard Bruce Wright                 2001    HarperFlamingo Canada  \n",
       "2          Carlo D'Este                 1991          HarperPerennial  \n",
       "3      Gina Bari Kolata                 1999     Farrar Straus Giroux  \n",
       "4       E. J. W. Barber                 1999   W. W. Norton & Company  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = pd.read_csv('Data\\datasets\\BX_Books_correct.csv', sep=';', names=i_cols, encoding='latin-1',low_memory=False)\n",
    "items = items.loc[1:]\n",
    "items.reset_index(drop=True, inplace=True)\n",
    "items.drop(['img_s','img_m','img_l'], axis=1, inplace=True)\n",
    "items.year_of_publication = items.year_of_publication.astype(int)\n",
    "items.describe(include =[object, int])\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">From the above we can see that : \n",
    "<ul style=\"font-size:12pt;\">\n",
    "<li> The \"isbn\" column has only unique values, as expected.</li>\n",
    "<li> There are duplicates in the \"book_titles\". Some might be indeed duplicate entries but some are just books with the same title. </li>\n",
    "<li>Agatha Christie is the most frequently appearing author. </li>\n",
    "<li>The min value in \"year_of publication\" is <b>0</b> and the max <b>2050 (?!?)</b>.</li>\n",
    "<li>There is one missing value in the \"book_author\" column and two in the \"publisher\".</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:20pt;\"> Replace NaN and incorrect \"year_of_publication\" values</h2>\n",
    "<p style=\"font-size:15pt;\">Starting with replacing the missing values, since the number of them is so small. I would prefer to replace them with the correct values rather than with \"other\" for example. Those items are:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items with NaN values in \"book_author\": \n",
      " 187700    9627982032\n",
      "Name: isbn, dtype: object \n",
      "\n",
      "Items values in \"publisher\": \n",
      " 128896    193169656X\n",
      "129043    1931696993\n",
      "Name: isbn, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (\"Items with NaN values in \\\"book_author\\\": \\n\", items.isbn[items.book_author.isna()],\"\\n\")\n",
    "print (\"Items values in \\\"publisher\\\": \\n\", items.isbn[items.publisher.isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">A search for the item missing the \"book_author\" yields an item with no author. The search for the books with no publishers shows that they both have the same publisher. So the following lines of code fill NaN values with the correct ones.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items.loc[187701,'book_author'] = \"n/a\"\n",
    "items.loc[[128897, 129044],'publisher'] = \"NovelBooks, Inc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">Next, we should search for the incorrect \"year_of_publication\" values. This dataset was created in 2004 so it would be logical to assume that values greater than that should be replaced. But then I took a closer look at some items and they were published in 2005 so, I guess, the dataset was updated. I am going to use 2010 as an upper limit and investigate further for the lower limit.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items with (year_of_publication > 2010): 20 \n",
      "\n",
      "value_counts of items with (year_of_publication < 1500): \n",
      " 0       4619\n",
      "1378       1\n",
      "1376       1\n",
      "Name: year_of_publication, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Items with (year_of_publication > 2010):', items.year_of_publication[items.year_of_publication>2010].count(),'\\n')\n",
    "print('value_counts of items with (year_of_publication < 1500): \\n', items.year_of_publication[items.year_of_publication<1500].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       year_of_publication\n",
      "count        266740.000000\n",
      "mean           1993.687062\n",
      "std               8.320673\n",
      "min            1376.000000\n",
      "25%            1989.000000\n",
      "50%            1996.000000\n",
      "75%            2000.000000\n",
      "max            2010.000000 \n",
      "\n",
      "count    271379.000000\n",
      "mean       1993.692412\n",
      "std           8.249348\n",
      "min        1376.000000\n",
      "25%        1989.000000\n",
      "50%        1995.000000\n",
      "75%        2000.000000\n",
      "max        2010.000000\n",
      "Name: year_of_publication, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "items.loc[(items.year_of_publication>2010)|(items.year_of_publication<1000),'year_of_publication'] = np.nan\n",
    "print(items.describe(),'\\n')\n",
    "print(items.year_of_publication.fillna(round(items.year_of_publication.mean())).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">The mean and the std don't change that much so I will use the mean as a replacement. If you want, you can try the method used in <em>\"users.age\"</em> or something else...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items.year_of_publication.fillna(round(items.year_of_publication.mean()),inplace=True)\n",
    "items.year_of_publication = items.year_of_publication.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:20pt;\"> Dealing with duplicate entries</h2>\n",
    "<p style=\"font-size:15pt;\">Moving on to duplicate detection of items entries, we saw before that there are 29225 duplicate book titles. That's counting only the ones that are exactly the same. Then again there might be some books with the exact same title but they might not be written by the same author, meaning <b>not</b> the same book (except if the author's name is written a bit differently so...). Now let's investigate how many items have the same title and author. They will be duplicate entries with great probability.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              isbn    book_title      book_author         publisher\n",
      "count        35921         35921            35921             35921\n",
      "unique       35921         15376             7694              2505\n",
      "top     0451151852  Little Women  Agatha Christie  Ballantine Books\n",
      "freq             1            21              259              1095 \n",
      "\n",
      "\n",
      "              isbn    book_title   book_author         publisher\n",
      "count        20175         20175         20175             20175\n",
      "unique       20175         15376          7694              2019\n",
      "top     0802140211  Little Women  Stephen King  Ballantine Books\n",
      "freq             1            20           192               590 \n",
      "\n",
      "\n",
      "Stephen King           192\n",
      "Agatha Christie        156\n",
      "Dick Francis            90\n",
      "Jane Austen             72\n",
      "William Shakespeare     71\n",
      "Name: book_author, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(items[(items.duplicated(['book_title', 'book_author'], keep=False))].describe(include=[object]),'\\n'*2)\n",
    "print(items[(items.duplicated(['book_title', 'book_author'], keep='first'))].describe(include=[object]),'\\n'*2)\n",
    "print(items[(items.duplicated(['book_title', 'book_author']))].book_author.value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">So, there are 35921 books with the same title and author (possibly different editions). If our goal is to build a <b>Book Recommender</b> then we care much more about the title rather than the specific editions. At least that's how I see it. That's why I believe we should delete all duplicates. From those 35921, if we keep the first occurrence, there will be 20175 redundant books. <br>But we must be <b>careful</b>! We have to transfer all the ratings of the deleted titles to the titles we will keep. We will do this later when we will process the <em>\"ratings\" Df</em>. For now let's create an <em>\"items_wo_duplicates\" Df</em>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>251204</td>\n",
       "      <td>251204</td>\n",
       "      <td>251203</td>\n",
       "      <td>251204.000000</td>\n",
       "      <td>251202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>251204</td>\n",
       "      <td>242154</td>\n",
       "      <td>102029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>155773058X</td>\n",
       "      <td>Selected Poems</td>\n",
       "      <td>William Shakespeare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Harlequin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993.705817</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.245138</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1376.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1989.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              isbn      book_title          book_author  year_of_publication  \\\n",
       "count       251204          251204               251203        251204.000000   \n",
       "unique      251204          242154               102029                  NaN   \n",
       "top     155773058X  Selected Poems  William Shakespeare                  NaN   \n",
       "freq             1              25                  496                  NaN   \n",
       "mean           NaN             NaN                  NaN          1993.705817   \n",
       "std            NaN             NaN                  NaN             8.245138   \n",
       "min            NaN             NaN                  NaN          1376.000000   \n",
       "25%            NaN             NaN                  NaN          1989.000000   \n",
       "50%            NaN             NaN                  NaN          1995.000000   \n",
       "75%            NaN             NaN                  NaN          2000.000000   \n",
       "max            NaN             NaN                  NaN          2008.000000   \n",
       "\n",
       "        publisher  \n",
       "count      251202  \n",
       "unique      16617  \n",
       "top     Harlequin  \n",
       "freq         7508  \n",
       "mean          NaN  \n",
       "std           NaN  \n",
       "min           NaN  \n",
       "25%           NaN  \n",
       "50%           NaN  \n",
       "75%           NaN  \n",
       "max           NaN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_wo_duplicates = items.drop_duplicates(['book_title', 'book_author'])\n",
    "items_wo_duplicates.describe(include=[object,int])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">The new \"items\" Df now has 9050 duplicate books but none of them is also from the same author. Agatha Christie is no more the most frequent author but William Shakespeare took her place.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<br><h2 style=\"font-size:25pt;\"><u>Ratings DataFrame</u></h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">The last DataFrame is <em>\"ratings\"</em> and consists of three columns. These are <em>\"user_id\", \"isbn\" and \"rating\".</em> <br>The ratings of each user are either <b>explicit</b> (1-10) meaning the user rated the item, or <b>implicit</b> (0) meaning the result of observed behavior.<br>First things first. As we saw earlier there are ratings which do not correspont to items in our dataset. We don't need those since we cannot recommend those items. We will create a \"ratings_new\" Df only with the ratings for the books in our dataset. And then we will convert the \"ratings.rating\" to 'int'. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1031175</td>\n",
       "      <td>1031175</td>\n",
       "      <td>1031175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>92107</td>\n",
       "      <td>270170</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>11676</td>\n",
       "      <td>0971880107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11144</td>\n",
       "      <td>2502</td>\n",
       "      <td>647323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id        isbn   rating\n",
       "count   1031175     1031175  1031175\n",
       "unique    92107      270170       11\n",
       "top       11676  0971880107        0\n",
       "freq      11144        2502   647323"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_new = ratings[ratings.isbn.isin(items.isbn)]\n",
    "ratings_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     647323\n",
      "1       1481\n",
      "2       2375\n",
      "3       5118\n",
      "4       7617\n",
      "5      45355\n",
      "6      31689\n",
      "7      66404\n",
      "8      91806\n",
      "9      60780\n",
      "10     71227\n",
      "Name: rating, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1031175</td>\n",
       "      <td>1031175</td>\n",
       "      <td>1.031175e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>92107</td>\n",
       "      <td>270170</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>11676</td>\n",
       "      <td>0971880107</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>11144</td>\n",
       "      <td>2502</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.839022e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.854149e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id        isbn        rating\n",
       "count   1031175     1031175  1.031175e+06\n",
       "unique    92107      270170           NaN\n",
       "top       11676  0971880107           NaN\n",
       "freq      11144        2502           NaN\n",
       "mean        NaN         NaN  2.839022e+00\n",
       "std         NaN         NaN  3.854149e+00\n",
       "min         NaN         NaN  0.000000e+00\n",
       "25%         NaN         NaN  0.000000e+00\n",
       "50%         NaN         NaN  0.000000e+00\n",
       "75%         NaN         NaN  7.000000e+00\n",
       "max         NaN         NaN  1.000000e+01"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_new.loc[:,'rating'] = ratings_new.rating.astype(int)\n",
    "print(ratings_new.rating.value_counts(sort=False))\n",
    "ratings_new.describe(include=[object,int])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">The thing that caught my eye immediately when looking at the \"ratings\" description is that more than 50% of all ratings are implicit.\n",
    "<br><br>But before we go any further we will have to transfer all ratings of duplicate titles and create the \"ratings_wo_duplicates\" Df. To do this you can choose <b>either</b> to import the '*.csv' file I have created <b>or</b> to reconstruct. <em>The latter might take a significant amount of time.</em></p>\n",
    "<p style=\"font-size:15pt\"> Thanks to PascPeli for the code below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose '1' to import the ratings_wo_duplicates file or '2' to construct it again 1\n",
      "Importing 'ratings_wo_duplicates.csv'\n",
      "Done\n",
      "\n",
      "And to make sure that the procedure was carried out smoothly,\n",
      "No of duplicates in \"ratings_wo_duplicates\" : 0\n"
     ]
    }
   ],
   "source": [
    "choice = input(\"Choose \\'1\\' to import the ratings_wo_duplicates file or \\'2\\' to construct it again \")\n",
    "while choice not in rightchoice:\n",
    "    choice = input(\"Wrong input. \\n Insert a number. Either 1 or 2\")\n",
    "\n",
    "if choice == '1':\n",
    "    print('Importing \\'ratings_wo_duplicates.csv\\'')\n",
    "    ratings_wo_duplicates=pd.read_csv('Data/datasets/ratings_wo_duplicates.csv', sep=';', names=r_cols, encoding='latin-1', low_memory=False )\n",
    "    print('Done')\n",
    "elif choice == '2':\n",
    "    print('Constructing \\'ratings_wo_duplicates.csv\\'')\n",
    "    print('Please remember the number of processed and stored items incase the operation is interupted and you would like continue from there.')\n",
    "    \n",
    "    choice = input(\"Choose \\'1\\' to iterate through all items or \\'2\\' if this operation was interupted and you would like to continue from the last checkpoint.\")\n",
    "    while choice not in rightchoice:\n",
    "        choice = input(\"Wrong input. \\n Insert a number. Either 1 or 2\")\n",
    "    \n",
    "    if choice == '1':\n",
    "        nof = 0\n",
    "        ratings_wo_duplicates = ratings_new\n",
    "        count=0\n",
    "    else:\n",
    "        nof = int(input('Please insert the number of processed and stored items.'))\n",
    "        ratings_wo_duplicates=pd.read_csv('Data/datasets/new datasets/ratings_wo_duplicates.csv', sep=';', names=r_cols, encoding='latin-1', low_memory=False )\n",
    "        count= nof\n",
    "    \n",
    "    # create a series with all the duplicates (including the first occurance) to iterate\n",
    "    temp=items[(items.duplicated(['book_title', 'book_author'],keep=False))][nof:]\n",
    "    \n",
    "    for t in tqdm.tqdm_notebook(temp['book_title']):\n",
    "        x = list( items[items['book_title']==t].isbn)\n",
    "        count+=1 \n",
    "        for i in range(1, len(x)):\n",
    "            #replace all entries in x list with x[0] (the isbn we kept in items_wo_duplicates)\n",
    "            ratings_wo_duplicates.loc[ratings_wo_duplicates.isbn==x[i],'isbn'] = x[0]\n",
    "\n",
    "        if count%2000==0:\n",
    "            ratings_wo_duplicates.to_csv('datasets/new datasets/ratings_wo_duplicates.csv',';', index=False)\n",
    "            print(count ,' duplicate items ratings processed and stored')\n",
    "            \n",
    "    ratings_wo_duplicates.to_csv('Data/datasets/new datasets/ratings_wo_duplicates.csv',';', index=False)\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "ratings_wo_duplicates = ratings_wo_duplicates.loc[1:]\n",
    "ratings_wo_duplicates.reset_index(drop=True, inplace=True)\n",
    "ratings_wo_duplicates.rating = ratings_wo_duplicates.rating.astype(int)\n",
    "print('\\nAnd to make sure that the procedure was carried out smoothly,')\n",
    "print('No of duplicates in \\\"ratings_wo_duplicates\\\" :',ratings_wo_duplicates.isbn[ratings_wo_duplicates.isbn.isin(items[items.duplicated(['book_title', 'book_author'])].isbn)].count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">The next move is to separate the explicit from the implicit ratings and place them into two Dataframes. Then create two new \"users\" Dfs containing the users who have rated an item either explicitly or implicitly accordingly.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings_expl = ratings_wo_duplicates[ratings_wo_duplicates.rating != 0]\n",
    "ratings_impl = ratings_wo_duplicates[ratings_wo_duplicates.rating == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id        isbn         rating\n",
      "count   383852      383852  383852.000000\n",
      "unique   68092      137646            NaN\n",
      "top      11676  0316666343            NaN\n",
      "freq      6943         707            NaN\n",
      "mean       NaN         NaN       7.626710\n",
      "std        NaN         NaN       1.841331\n",
      "min        NaN         NaN       1.000000\n",
      "25%        NaN         NaN       7.000000\n",
      "50%        NaN         NaN       8.000000\n",
      "75%        NaN         NaN       9.000000\n",
      "max        NaN         NaN      10.000000 \n",
      "\n",
      "       user_id        isbn    rating\n",
      "count   647322      647322  647322.0\n",
      "unique   52451      184258       NaN\n",
      "top     198711  0971880107       NaN\n",
      "freq      6439        1921       NaN\n",
      "mean       NaN         NaN       0.0\n",
      "std        NaN         NaN       0.0\n",
      "min        NaN         NaN       0.0\n",
      "25%        NaN         NaN       0.0\n",
      "50%        NaN         NaN       0.0\n",
      "75%        NaN         NaN       0.0\n",
      "max        NaN         NaN       0.0\n"
     ]
    }
   ],
   "source": [
    "print(ratings_expl.describe(include=[object,int]),'\\n')\n",
    "print(ratings_impl.describe(include=[object,int]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_w_ex_ratings = users_new[users_new.user_id.isin(ratings_expl.user_id)]\n",
    "users_w_im_ratings = users_new[users_new.user_id.isin(ratings_impl.user_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">I don't think there is much reason doing the same with the \"items\" dataframe but I have included the code anyway in the cell below.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items_w_ratings = items_wo_duplicates[items_wo_duplicates.isbn.isin(ratings_wo_duplicates.isbn)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:25pt;\">Final words</h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">Finaly, we end up with 7 new Dataframes.</p>\n",
    "<ul style=\"font-size:13pt;\">\n",
    "<li><b>users_new:</b> Users dataframe without NaN values and split location strings or word embeddings.</li>\n",
    "<li><b>users_w_ex_ratings:</b> Users who have contributed at least one explicit rating</li>\n",
    "<li><b>users_w_im_ratings:</b> Users who have contributed at least one implicit rating</li>\n",
    "<li><b>items_wo_duplicates:</b> Items dataframe without double entries or NaN values</li>\n",
    "<li><b>ratings_wo_duplicates:</b> Ratings corresponding to the items_wo_duplicates dataframe. All ratings kept and \"transferred\".</li>\n",
    "<li><b>ratings_expl:</b> Dataframe with the explicit ratings</li>\n",
    "<li><b>ratings_impl:</b> Dataframe with the implicit ratings</li>\n",
    "</ul>\n",
    "<br>\n",
    "<p style=\"font-size:15pt;\">The last step would be to save them for later use.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_new.to_csv('Data/datasets/new datasets/users_new.csv',';', index=False)\n",
    "users_w_ex_ratings.to_csv('Data/datasets/new datasets/users_w_ex_ratings.csv',';', index=False)\n",
    "users_w_im_ratings.to_csv('Data/datasets/new datasets/users_w_im_ratings.csv',';', index=False)\n",
    "items_wo_duplicates.to_csv('Data/datasets/new datasets/items_wo_duplicates.csv',';', index=False)\n",
    "ratings_wo_duplicates.to_csv('Data/datasets/new datasets/ratings_wo_duplicates.csv',';', index=False)\n",
    "ratings_expl.to_csv('Data/datasets/new datasets/ratings_expl.csv',';', index=False)\n",
    "ratings_impl.to_csv('Data/datasets/new datasets/ratings_impl.csv',';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br>\n",
    "<p style=\"font-size:15pt;\">What we have to do in order to produce item recommendations would be to create the users/ratings matrix containing the ratings of each user to each rated item. Then, compute the distances or similarities between each user for a User-based collaborative filtering or between each item for an Item-based collaborative filtering... Of course, this dataset is huge so those matrices would be enormous and memory heavy so it would be better to do it in batches. \n",
    "<br><br>\n",
    "If you feel like dropping a line for any questions or suggestions, don't hesitate to email me at <a href=\"mailto:ecnnoruka@yahoo.com?Subject=Concerning%Recommendation-System-for-Book-Crossing%20Notebook\" target=\"_top\">ecnnoruka@yahoo.com</a> or my mentor at <a href=\"mailto:ppelitaris@gmail.com?Subject=Consernig%20BookRecommendation%20Notebook\" target=\"_top\">ppelitaris@gmail.com</a>\n",
    "</p> \n",
    "<h2 style=\"font-size:25pt;\"align=right>To be continued...</h2>\n",
    "<pt>P.P.</p>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
